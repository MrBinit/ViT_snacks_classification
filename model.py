# -*- coding: utf-8 -*-
"""data_preparation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yS8S_GvF6hD0dYIgcaTIXYckwf7h-4-y
"""

# from google.colab import drive
# drive.mount('/content/drive')

from datasets import load_dataset

# load dataset
dataset = load_dataset("Matthijs/snacks")
print(dataset)

print(dataset['train'].features['label'].names)

# Mapping from label to index and vice versa
labels = dataset['train'].features['label'].names
num_labels = len(dataset['train'].features['label'].names)
label2id, id2label = dict(), dict()
for i, label in enumerate(labels):
    label2id[label] = i
    id2label[i] = label

print(label2id)
print(id2label)

"""Project our input image into a convolutional layer with the kernel size and stride equal to the patch size. Then, we flatten the output from that convolutional layer."""

import torch
import torch.nn as nn
# create toy image with dims (batch x channel x width x height)
toy_img = torch.rand(1, 3, 48, 48)
# define conv layer parameters
nums_channels = 3
hidden_size = 768 #the size of these token embeddings
patch_size = 16

#Conv 2D layer
projection = nn.Conv2d(nums_channels, hidden_size, kernel_size= patch_size, stride= patch_size)

#forward pass toy img
out_projection = projection(toy_img)

print(f'Original image size: {toy_img.size()}')
print(f'Size after projection: {out_projection.size()}')

"""The "emb_dimension" parameter determines the size of these token embeddings. A larger "emb_dimension" implies that each token embedding will have a higher dimensionality, potentially capturing more complex features from the input image patches."""

patch_embeddings = out_projection.flatten(2).transpose(1,2)
print(f'Patch embedding size: {patch_embeddings.size()}')

batch_size = 1
cls_token = nn.Parameter(torch.randn(1, 1, hidden_size))
cls_tokens = cls_token.expand(batch_size, -1 , -1)

#prepend [CLS] token in the beginning of patch embedding
patch_embeddings = torch.cat((cls_tokens, patch_embeddings), dim=1)
print(f'Patch embedding size: {patch_embeddings.size()}')

# define position embedding with the same dimension as the patch embedding

position_embeddings = nn.Parameter(torch.randn(batch_size, 10, hidden_size))

#add position embedding into patch embedding
input_embeddings = patch_embeddings + position_embeddings
print(f'Input embedding size: {input_embeddings.size()}')

num_heads = 12
num_layers = 12

# define transformer encoders' stack

transformer_encoder_layer = nn.TransformerEncoderLayer(
    d_model= hidden_size, nhead= num_heads, dim_feedforward= int(hidden_size * 4), dropout= 0.1)
transformer_encoder = nn.TransformerEncoder(encoder_layer= transformer_encoder_layer, num_layers= num_layers)

output_embedding = transformer_encoder(input_embeddings)
print(f' Output embedding size: {output_embedding.size()}')

from transformers import ViTModel

#load pretrained model
model_checkpoint = 'google/vit-base-patch16-224-in21k'
model = ViTModel.from_pretrained(model_checkpoint, add_pooling_layer = False)

#example input image

input_img = torch.rand(batch_size, nums_channels, 224, 224)

# Forward pass input image
output_embedding = model(input_img)
print(output_embedding)
print(f"Output embedding size: {output_embedding['last_hidden_state'].size()}")

num_labels = 20

# Define linear classifier layer
classifier = nn.Linear(hidden_size, num_labels)

# Forward pass on the output embedding of [CLS] token
output_classification = classifier(output_embedding['last_hidden_state'][:, 0, :])
print(f"Output embedding size: {output_classification.size()}")

import numpy as np
import torch
import cv2
import torch.nn as nn
from transformers import ViTModel, ViTConfig
from torchvision import transforms
from torch.optim import Adam
from torch.utils.data import DataLoader
from tqdm import tqdm

#Pretrained model checkpoint
model_checkpoint = 'google/vit-base-patch16-224-in21k'

class ImageDataset(torch.utils.data.Dataset):

  def __init__(self, input_data):

      self.input_data = input_data
      # Transform input data
      self.transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize((224, 224), antialias=True),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
        ])

  def __len__(self):
      return len(self.input_data)

  def get_images(self, idx):
      return self.transform(self.input_data[idx]['image'])

  def get_labels(self, idx):
      return self.input_data[idx]['label']

  def __getitem__(self, idx):
      # Get input data in a batch
      train_images = self.get_images(idx)
      train_labels = self.get_labels(idx)

      return train_images, train_labels

class ViT(nn.Module):

  def __init__(self, config=ViTConfig(), num_labels=20,
               model_checkpoint='google/vit-base-patch16-224-in21k'):

        super(ViT, self).__init__()

        self.vit = ViTModel.from_pretrained(model_checkpoint, add_pooling_layer=False)
        self.classifier = (
            nn.Linear(config.hidden_size, num_labels)
        )

  def forward(self, x):

    x = self.vit(x)['last_hidden_state']
    # Use the embedding of [CLS] token
    output = self.classifier(x[:, 0, :])

    return output

def model_train(dataset, epochs, learning_rate, bs):

    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

    # Load nodel, loss function, and optimizer
    model = ViT().to(device)
    criterion = nn.CrossEntropyLoss().to(device)
    optimizer = Adam(model.parameters(), lr=learning_rate)

    # Load batch image
    train_dataset = ImageDataset(dataset)
    train_dataloader = DataLoader(train_dataset, num_workers=1, batch_size=bs, shuffle=True)

    # Fine tuning loop
    for i in range(epochs):
        total_acc_train = 0
        total_loss_train = 0.0

        for train_image, train_label in tqdm(train_dataloader):
            output = model(train_image.to(device))
            loss = criterion(output, train_label.to(device))
            acc = (output.argmax(dim=1) == train_label.to(device)).sum().item()
            total_acc_train += acc
            total_loss_train += loss.item()

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        print(f'Epochs: {i + 1} | Loss: {total_loss_train / len(train_dataset): .3f} | Accuracy: {total_acc_train / len(train_dataset): .3f}')
            # Save the trained model
    torch.save(model.state_dict(), '/content/drive/My Drive/trained_model.pth')

    return model

# Hyperparameters
EPOCHS = 10
LEARNING_RATE = 1e-4
BATCH_SIZE = 8

# Train the model
trained_model = model_train(dataset['train'], EPOCHS, LEARNING_RATE, BATCH_SIZE)

def predict(img):

    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize((224, 224)),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
        ])

    img = transform(img)
    output = trained_model(img.unsqueeze(0).to(device))
    prediction = output.argmax(dim=1).item()

    return id2label[prediction]

print(predict(dataset['test'][900]['image']))

from PIL import Image

def predict(img_path):
    # Load image
    img = Image.open(img_path).convert('RGB')

    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize((224, 224)),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
        ])

    img = transform(img)
    output = trained_model(img.unsqueeze(0).to(device))
    prediction = output.argmax(dim=1).item()

    return id2label[prediction]

# Example
predicted_class = predict("/content/drive/My Drive/cake.jpg")
print("Predicted class:", predicted_class)

from PIL import Image
import matplotlib.pyplot as plt


def predict(img_path):
    # Load image
    img = Image.open(img_path).convert('RGB')

    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize((224, 224)),
        transforms.Normalize(mean=[0.5, 0.5, 0.5],
                             std=[0.5, 0.5, 0.5])
        ])

    img = transform(img)
    output = trained_model(img.unsqueeze(0).to(device))
    prediction = output.argmax(dim=1).item()

    return id2label[prediction]


# # Open the image
# img = Image.open("/content/drive/My Drive/cookies.jpeg")

# # Display the image
# plt.imshow(img)
# plt.axis('off')  # Turn off axis labels
# plt.show()

# # Example
# predicted_class = predict("/content/drive/My Drive/cookies.jpeg")
# print("Predicted class:", predicted_class)

# # Open the image
# img = Image.open("/content/drive/My Drive/images.jpeg")

# # Display the image
# plt.imshow(img)
# plt.axis('off')  # Turn off axis labels
# plt.show()
# # Example
# predicted_class = predict("/content/drive/My Drive/images.jpeg")
# print("Predicted class:", predicted_class)

# # Open the image
# img = Image.open("/content/drive/My Drive/images_modi .jpeg")

# # Display the image
# plt.imshow(img)
# plt.axis('off')  # Turn off axis labels
# plt.show()
# # Example
# predicted_class = predict("/content/drive/My Drive/images_modi .jpeg")
# print("Predicted class:", predicted_class)

# # Open the image
# img = Image.open("/content/drive/My Drive/juice.jpeg")

# # Display the image
# plt.imshow(img)
# plt.axis('off')  # Turn off axis labels
# plt.show()
# # Example
# predicted_class = predict("/content/drive/My Drive/juice.jpeg")
# print("Predicted class:", predicted_class)

# # Open the image
# img = Image.open("/content/drive/My Drive/waffles_1.jpeg")

# # Display the image
# plt.imshow(img)
# plt.axis('off')  # Turn off axis labels
# plt.show()
# # Example
# predicted_class = predict("/content/drive/My Drive/waffles_1.jpeg")
# print("Predicted class:", predicted_class)

